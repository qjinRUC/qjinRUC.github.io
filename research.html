<html>
<head>
	<meta http-equiv="content-type" content ="text/html" charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Qin Jin</title>
	<link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
	<link href="css/style.css" rel="stylesheet">
	<link href="http://fonts.googleapis.com/css?family=Roboto:400,300,500" rel="stylesheet">
	
</head>
<body>
	<div class="container col-md-8 col-md-offset-2">
		<div class="page-header media">
			<div class="col-md-3 col-xs-4">
				<h1 class="title">Qin Jin</h1>
			</div>
			<div class="col-md-9 col-xs-14">
				<h1><small class="subtitle">
					Associate Professor<br>
					School of Infomation<br>
					Renmin University of China
				</small></h1>
			</div>
		</div>
		<nav>
			<ul id="menu" class="list-unstyled pull-right">
				<li><a href="index.html">Home</a></li>
				<li><a href="research.html" class="menu-active">Research</a></li>
				<li><a href="publications.html">Publications</a></li>
				<li><a href="awards-services.html">Awards/Services</a></li>
				<li><a href="contact.html">Contact</a></li>
			</ul>
		</nav>
	</div>
	
	<div class="container col-md-8 col-md-offset-2">
		<h3 class="subheader">Research Overview</h3>
		<ul class="list-unstyled">
		
		<!--
		<li class="research-item">
			<h4><img src="images/punt_gran.jpg">
				title ...
			</br><img src="images/line_p3.jpg" width="100%"></h4>
			<div class="media">
				<div class="media-left">
					<img class="media-object" src="path/to/img" width="220px">
				</div>
				<div class="media-body">
					<p>
						content ...
					</p> 
				</div>
			</div>
		</li>
		-->
                <li class="research-item">
			<h4><img src="images/punt_gran.jpg">
				<span class="redcolor">I</span>mage/<span class="redcolor">V</span>ideo <span class="redcolor">D</span>escription with <span class="redcolor">N</span>atural <span class="redcolor">L</span>anguage
			</br><img src="images/line_p3.jpg" width="100%"></h4>
			<div class="media">
				<div class="media-left">
					<img class="media-object" src="images/framework.png" width="220px">
					<font size="1"> <center> Fig.1 The TGM Framework.</center></font>
					</br> 
					<img class="media-object" src="images/model_cases.png" width="220px">
					<font size="1"> <center>Fig.2 The Cases of the Models.</center></font>
				</div>
				<div class="media-body">
					<p>
					Generating natural language descriptions of visual content is an intriguing task. It has a wide range of applications such as text summarization for video preview, assisting blind people, or improving search quality for online videos. Our works focus on the following four main challenges for video captioning. </br> 1) <b>Multi-modalities</b>. Different from images, a video consists of multi-modalities, such as visual and aural modality. Different modalities are complementary with each other. In our work, we extract multimodal features from image, motion, acoustic and speech and explore different multimodal fusion methods to generate better video representations. </br>2) <b>Temporal movements</b>. The objects' movements on time demension make video captioning more challenging than image captioning task. We explore the sequence models to encode temporal video contents and utilize the temporal attention mechanism to dynamically pay attention to different related segments when generating the captions. </br>3) <b>Diverse topics</b>. The topics of open-domain videos are quite diverse such as sports, cooking, news and so on. The vocabularies and expression styles for different topics vary a lot, which makes it hard to capture the complex sentence distribution. We propose the topic-guided caption model (TGM) with the guidance of automatically mined latent topics. The TGM can generate more accurate and detailed video descriptions and achieve the state-of-the-art performance on different captioning datasets.</br> 4) <b>Wisdom of all</b>. Different models are complementary with each other. Therefore, we propose the ensemble and rerank strategy to assemble the wisdom of different models via modifying or selecting the best video caption.
					</p> 
				</div>
			</div>
		</li>
		
		<li class="research-item">
			<h4><img src="images/punt_gran.jpg">
				<span class="redcolor">M</span>ultimodal <span class="redcolor">E</span>motion <span class="redcolor">A</span>nalysis
			</br><img src="images/line_p3.jpg" width="100%"></h4>
			<div class="media">
				<div class="media-left">
					<img class="media-object" src="images/multimodal-Emotion.png" width="220px">
				</div>
				<div class="media-body">
					<p>
						Understanding human emotions is an important step to build natural human-computer interfaces. A wide range of applications is emerging in marketing, education, health care and entertainment that can benefit from the automatic emotion recognition. For example, service robots can improve the customer satisfaction by adjusting their reactions towards the customersâ€™ emotion states; patients with mental diseases could be better treated with the virtual emotional companion.
						<br><br>
						Our researches focus on the emotion recognition problem with the two leading emotion models in cognitive science: the categorical emotion and the dimensional emotion. Since emotions are conveyed through multiple human behaviors, we develop our emotion recognition system using multiple modalities such as speech, verbal content, facial expression, and body movement. We explore the unimodal discriminative features, temporal dynamic of emotions and multimodal fusion strategies. Our goal is to build artificial intelligence with high emotional quotation.
					</p> 
				</div>
			</div>
		</li>
		
		<li class="research-item">
			<h4><img src="images/punt_gran.jpg">
				<span class="redcolor">M</span>ultimedia <span class="redcolor">P</span>rofiling for <span class="redcolor">H</span>istoric <span class="redcolor">E</span>vents
			</br><img src="images/line_p3.jpg" width="100%"></h4>
			<div class="media">
				<div class="media-left">
					<img class="media-object" src="images/history1.bmp" width="350px">
				</div>
				<div class="media-body">
					<p>
						History event related knowledge is precious and multimedia such as imagery is a powerful medium that records diverse information about the event. In this work, we automatically construct an image/multimedia profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. Furthermore, we automatically add explicit semantic information to image profiling by linking images in the profile with related phrases in the event description.
					</p> 
				</div>
				<br>
				<div class="row">
					<div class="col-md-6 col-xs-9">
						<img class="img-responsive" src="images/history2.bmp" width="100%">
					</div>
					<div class="col-md-6 col-xs-9">
						<img class="img-responsive" src="images/history3.png" width="100%">
					</div>
				</div>
			</div>
		</li>
		</ul>
	
		<br><br><br>
	</div>
	
	
	<script src="js/jquery.min.js"></script>
	<script src="js/bootstrap.min.js"></script>
</body>
</html>
